{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, absolute_import, print_function\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import warnings\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from detect.util import (get_data, get_mc_predictions, normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_resize(X,batch_size):\n",
    "    n_batches = int(np.ceil(X.shape[0] / float(batch_size)))\n",
    "    ls=[]\n",
    "    for i in range(n_batches):\n",
    "        X_batch = X[i * batch_size:(i + 1) * batch_size]\n",
    "        new_X = np.transpose(X_batch.squeeze(axis=-1),(1,2,0))\n",
    "        #X_center = cv2.resize(new_X[4:24,4:24,:],dsize=(28,28),interpolation=cv2.INTER_CUBIC)\n",
    "        X_center = cv2.resize(new_X[1:-1,1:-1,:],dsize=(28,28),interpolation=cv2.INTER_CUBIC)\n",
    "        X_left_top = cv2.resize(new_X[:27,:27,:],dsize=(28,28),interpolation=cv2.INTER_CUBIC)\n",
    "        X_left_bot = cv2.resize(new_X[1:,:27,:],dsize=(28,28),interpolation=cv2.INTER_CUBIC)\n",
    "        X_right_top = cv2.resize(new_X[:27,1:,:],dsize=(28,28),interpolation=cv2.INTER_CUBIC)\n",
    "        X_right_bot = cv2.resize(new_X[1:,1:,:],dsize=(28,28),interpolation=cv2.INTER_CUBIC)\n",
    "        X_proc = np.asarray([X_center,X_left_top,X_left_bot,X_right_top,X_right_bot])\n",
    "        X_proc = np.transpose(X_proc,(0,3,1,2))\n",
    "        ls.append(X_proc)\n",
    "    X_proc=np.concatenate(ls,axis=1)\n",
    "    return np.expand_dims(X_proc,axis=-1)\n",
    "\n",
    "def preprocess_infer(X, preds, model, batch_size, C):\n",
    "    X_proc = crop_resize(X,batch_size)\n",
    "    label = np.zeros(preds.shape[0])\n",
    "    ct = np.zeros(preds.shape[0])\n",
    "    for i in range(5):\n",
    "        x = X_proc[i]\n",
    "        new_preds = model.predict_classes(x, verbose=0, batch_size=batch_size)\n",
    "        same_or_not = new_preds==preds\n",
    "        ct += same_or_not\n",
    "    label[np.where(ct>=C)[0]] = 1\n",
    "\n",
    "    return label\n",
    "\n",
    "def detect_clean_adv(X, preds, uncert, model, C, H, L, batch_size):\n",
    "    label = -1 * np.ones(preds.shape[0])\n",
    "    #print(label.shape[0])\n",
    "    label[np.where(uncert>H)[0]] = 0\n",
    "    #middle_idx = np.where(label == -1)[0]\n",
    "    #print(middle_idx.shape[0])\n",
    "    label[np.where(uncert<L)[0]] = 1\n",
    "    middle_idx = np.where(label == -1)[0]\n",
    "    #print(middle_idx.shape[0])\n",
    "    if middle_idx.shape[0] != 0:\n",
    "        label[middle_idx] = preprocess_infer(X[middle_idx], preds[middle_idx], model, batch_size, C)\n",
    "    return label\n",
    "\n",
    "def detect_clean_adv_v2(X, preds, uncert, model, C, H, L, batch_size):\n",
    "    label = -1 * np.ones(preds.shape[0])\n",
    "    #print(label.shape[0])\n",
    "    label[np.where(uncert>H)[0]] = 0\n",
    "    #middle_idx = np.where(label == -1)[0]\n",
    "    #print(middle_idx.shape[0])\n",
    "    label[np.where(uncert<=H)[0]] = 1\n",
    "    #middle_idx = np.where(label == -1)[0]\n",
    "    #print(middle_idx.shape[0])\n",
    "    #if middle_idx.shape[0] != 0:\n",
    "    #    label[middle_idx] = preprocess_infer(X[middle_idx], preds[middle_idx], model, batch_size, C)\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed: 0\n",
      "Loading the data and model...\n",
      "(60000, 28, 28, 1)\n",
      "(60000, 10)\n",
      "(10000, 28, 28, 1)\n",
      "(10000, 10)\n",
      "Loading adversarial samples...\n",
      "Model accuracy on the normal test set: 99.02%\n",
      "Model accuracy on the adversarial test set: 17.44%\n",
      "Model accuracy on the combined undefended test set: 58.23%\n",
      "Getting Monte Carlo dropout variance predictions...\n",
      "Seed: 0, C: 5, H: 0.0007512824086006731, L: 2.7283624149276875e-05\n",
      "Detection Acc: 92.2%, False Negative: 539, False Positive: 231, True Negative: 4720, True Positive: 4412.\n",
      "Computing final model predictions...\n",
      "Reclassification Acc: 95.3%, Clean Correct Acc: 95.0%, Clean Incorrect Acc: 0.0%, Adv Correct Acc: 0.30%, Adv Incorrect Acc: 4.67%.\n",
      "Seed: 0, C: 5, H: 0.002, L: 3e-06\n",
      "Detection Acc: 90.7%, False Negative: 379, False Positive: 532, True Negative: 4419, True Positive: 4572.\n",
      "Computing final model predictions...\n",
      "Reclassification Acc: 91.2%, Clean Correct Acc: 89.5%, Clean Incorrect Acc: 0.0%, Adv Correct Acc: 1.64%, Adv Incorrect Acc: 8.77%.\n",
      "seed: 1\n",
      "Loading the data and model...\n",
      "(60000, 28, 28, 1)\n",
      "(60000, 10)\n",
      "(10000, 28, 28, 1)\n",
      "(10000, 10)\n",
      "Loading adversarial samples...\n",
      "Model accuracy on the normal test set: 98.98%\n",
      "Model accuracy on the adversarial test set: 18.08%\n",
      "Model accuracy on the combined undefended test set: 58.53%\n",
      "Getting Monte Carlo dropout variance predictions...\n",
      "Seed: 1, C: 5, H: 0.0006294608756434172, L: 4.699101555161178e-05\n",
      "Detection Acc: 92.0%, False Negative: 591, False Positive: 195, True Negative: 4754, True Positive: 4358.\n",
      "Computing final model predictions...\n",
      "Reclassification Acc: 95.7%, Clean Correct Acc: 95.7%, Clean Incorrect Acc: 0.0%, Adv Correct Acc: 0.06%, Adv Incorrect Acc: 4.21%.\n",
      "Seed: 1, C: 5, H: 0.002, L: 3e-06\n",
      "Detection Acc: 90.9%, False Negative: 377, False Positive: 514, True Negative: 4435, True Positive: 4572.\n",
      "Computing final model predictions...\n",
      "Reclassification Acc: 91.8%, Clean Correct Acc: 89.8%, Clean Incorrect Acc: 0.0%, Adv Correct Acc: 1.94%, Adv Incorrect Acc: 8.15%.\n",
      "seed: 2\n",
      "Loading the data and model...\n",
      "(60000, 28, 28, 1)\n",
      "(60000, 10)\n",
      "(10000, 28, 28, 1)\n",
      "(10000, 10)\n",
      "Loading adversarial samples...\n",
      "Model accuracy on the normal test set: 99.18%\n",
      "Model accuracy on the adversarial test set: 18.66%\n",
      "Model accuracy on the combined undefended test set: 58.92%\n",
      "Getting Monte Carlo dropout variance predictions...\n",
      "Seed: 2, C: 5, H: 0.0005583128659054637, L: 1.962706392077962e-05\n",
      "Detection Acc: 92.2%, False Negative: 577, False Positive: 193, True Negative: 4766, True Positive: 4382.\n",
      "Computing final model predictions...\n",
      "Reclassification Acc: 96.0%, Clean Correct Acc: 95.7%, Clean Incorrect Acc: 0.0%, Adv Correct Acc: 0.24%, Adv Incorrect Acc: 3.97%.\n",
      "Seed: 2, C: 5, H: 0.002, L: 3e-06\n",
      "Detection Acc: 91.2%, False Negative: 358, False Positive: 508, True Negative: 4451, True Positive: 4601.\n",
      "Computing final model predictions...\n",
      "Reclassification Acc: 91.6%, Clean Correct Acc: 90.0%, Clean Incorrect Acc: 0.0%, Adv Correct Acc: 1.60%, Adv Incorrect Acc: 8.33%.\n",
      "seed: 3\n",
      "Loading the data and model...\n",
      "(60000, 28, 28, 1)\n",
      "(60000, 10)\n",
      "(10000, 28, 28, 1)\n",
      "(10000, 10)\n",
      "Loading adversarial samples...\n",
      "Model accuracy on the normal test set: 99.24%\n",
      "Model accuracy on the adversarial test set: 18.58%\n",
      "Model accuracy on the combined undefended test set: 58.91%\n",
      "Getting Monte Carlo dropout variance predictions...\n",
      "Seed: 3, C: 5, H: 0.0006455654802266508, L: 2.945647884189384e-05\n",
      "Detection Acc: 92.1%, False Negative: 583, False Positive: 197, True Negative: 4765, True Positive: 4379.\n",
      "Computing final model predictions...\n",
      "Reclassification Acc: 95.7%, Clean Correct Acc: 95.6%, Clean Incorrect Acc: 0.0%, Adv Correct Acc: 0.06%, Adv Incorrect Acc: 4.23%.\n",
      "Seed: 3, C: 5, H: 0.002, L: 3e-06\n",
      "Detection Acc: 90.9%, False Negative: 374, False Positive: 526, True Negative: 4436, True Positive: 4588.\n",
      "Computing final model predictions...\n",
      "Reclassification Acc: 91.3%, Clean Correct Acc: 89.7%, Clean Incorrect Acc: 0.0%, Adv Correct Acc: 1.62%, Adv Incorrect Acc: 8.66%.\n",
      "seed: 4\n",
      "Loading the data and model...\n",
      "(60000, 28, 28, 1)\n",
      "(60000, 10)\n",
      "(10000, 28, 28, 1)\n",
      "(10000, 10)\n",
      "Loading adversarial samples...\n",
      "Model accuracy on the normal test set: 99.24%\n",
      "Model accuracy on the adversarial test set: 17.12%\n",
      "Model accuracy on the combined undefended test set: 58.18%\n",
      "Getting Monte Carlo dropout variance predictions...\n",
      "Seed: 4, C: 5, H: 0.001825281826313585, L: 0.0004339494771556929\n",
      "Detection Acc: 91.1%, False Negative: 390, False Positive: 484, True Negative: 4478, True Positive: 4572.\n",
      "Computing final model predictions...\n",
      "Reclassification Acc: 91.8%, Clean Correct Acc: 90.4%, Clean Incorrect Acc: 0.0%, Adv Correct Acc: 1.40%, Adv Incorrect Acc: 8.16%.\n",
      "Seed: 4, C: 5, H: 0.002, L: 3e-06\n",
      "Detection Acc: 90.9%, False Negative: 374, False Positive: 528, True Negative: 4434, True Positive: 4588.\n",
      "Computing final model predictions...\n",
      "Reclassification Acc: 91.3%, Clean Correct Acc: 89.6%, Clean Incorrect Acc: 0.0%, Adv Correct Acc: 1.66%, Adv Incorrect Acc: 8.65%.\n",
      "seed: 5\n",
      "Loading the data and model...\n",
      "(60000, 28, 28, 1)\n",
      "(60000, 10)\n",
      "(10000, 28, 28, 1)\n",
      "(10000, 10)\n",
      "Loading adversarial samples...\n",
      "Model accuracy on the normal test set: 99.08%\n",
      "Model accuracy on the adversarial test set: 18.44%\n",
      "Model accuracy on the combined undefended test set: 58.76%\n",
      "Getting Monte Carlo dropout variance predictions...\n",
      "Seed: 5, C: 5, H: 0.0006597075844183564, L: 2.7015730665880255e-05\n",
      "Detection Acc: 91.9%, False Negative: 597, False Positive: 199, True Negative: 4755, True Positive: 4357.\n",
      "Computing final model predictions...\n",
      "Reclassification Acc: 95.7%, Clean Correct Acc: 95.6%, Clean Incorrect Acc: 0.0%, Adv Correct Acc: 0.13%, Adv Incorrect Acc: 4.23%.\n",
      "Seed: 5, C: 5, H: 0.002, L: 3e-06\n",
      "Detection Acc: 91.1%, False Negative: 387, False Positive: 485, True Negative: 4469, True Positive: 4567.\n",
      "Computing final model predictions...\n",
      "Reclassification Acc: 91.9%, Clean Correct Acc: 90.3%, Clean Incorrect Acc: 0.0%, Adv Correct Acc: 1.58%, Adv Incorrect Acc: 8.01%.\n",
      "seed: 6\n",
      "Loading the data and model...\n",
      "(60000, 28, 28, 1)\n",
      "(60000, 10)\n",
      "(10000, 28, 28, 1)\n",
      "(10000, 10)\n",
      "Loading adversarial samples...\n",
      "Model accuracy on the normal test set: 99.16%\n",
      "Model accuracy on the adversarial test set: 18.66%\n",
      "Model accuracy on the combined undefended test set: 58.91%\n",
      "Getting Monte Carlo dropout variance predictions...\n",
      "Seed: 6, C: 5, H: 0.0007059899508021772, L: 4.973698924004566e-05\n",
      "Detection Acc: 92.2%, False Negative: 547, False Positive: 217, True Negative: 4741, True Positive: 4411.\n",
      "Computing final model predictions...\n",
      "Reclassification Acc: 95.3%, Clean Correct Acc: 95.3%, Clean Incorrect Acc: 0.0%, Adv Correct Acc: 0.08%, Adv Incorrect Acc: 4.60%.\n",
      "Seed: 6, C: 5, H: 0.002, L: 3e-06\n",
      "Detection Acc: 91.0%, False Negative: 359, False Positive: 526, True Negative: 4432, True Positive: 4599.\n",
      "Computing final model predictions...\n",
      "Reclassification Acc: 91.2%, Clean Correct Acc: 89.7%, Clean Incorrect Acc: 0.0%, Adv Correct Acc: 1.54%, Adv Incorrect Acc: 8.72%.\n",
      "seed: 7\n",
      "Loading the data and model...\n",
      "(60000, 28, 28, 1)\n",
      "(60000, 10)\n",
      "(10000, 28, 28, 1)\n",
      "(10000, 10)\n",
      "Loading adversarial samples...\n",
      "Model accuracy on the normal test set: 99.12%\n",
      "Model accuracy on the adversarial test set: 17.60%\n",
      "Model accuracy on the combined undefended test set: 58.36%\n",
      "Getting Monte Carlo dropout variance predictions...\n",
      "Seed: 7, C: 5, H: 0.0005955217347946018, L: 3.3599410016904585e-05\n",
      "Detection Acc: 91.8%, False Negative: 597, False Positive: 210, True Negative: 4746, True Positive: 4359.\n",
      "Computing final model predictions...\n",
      "Reclassification Acc: 95.5%, Clean Correct Acc: 95.4%, Clean Incorrect Acc: 0.0%, Adv Correct Acc: 0.15%, Adv Incorrect Acc: 4.44%.\n",
      "Seed: 7, C: 5, H: 0.002, L: 3e-06\n",
      "Detection Acc: 91.0%, False Negative: 380, False Positive: 507, True Negative: 4449, True Positive: 4576.\n",
      "Computing final model predictions...\n",
      "Reclassification Acc: 91.3%, Clean Correct Acc: 90.0%, Clean Incorrect Acc: 0.0%, Adv Correct Acc: 1.33%, Adv Incorrect Acc: 8.63%.\n",
      "seed: 8\n",
      "Loading the data and model...\n",
      "(60000, 28, 28, 1)\n",
      "(60000, 10)\n",
      "(10000, 28, 28, 1)\n",
      "(10000, 10)\n",
      "Loading adversarial samples...\n",
      "Model accuracy on the normal test set: 98.94%\n",
      "Model accuracy on the adversarial test set: 19.04%\n",
      "Model accuracy on the combined undefended test set: 58.99%\n",
      "Getting Monte Carlo dropout variance predictions...\n",
      "Seed: 8, C: 5, H: 0.0006317190418485552, L: 3.553587703208905e-05\n",
      "Detection Acc: 92.3%, False Negative: 570, False Positive: 186, True Negative: 4761, True Positive: 4377.\n",
      "Computing final model predictions...\n",
      "Reclassification Acc: 96.0%, Clean Correct Acc: 95.9%, Clean Incorrect Acc: 0.0%, Adv Correct Acc: 0.13%, Adv Incorrect Acc: 3.94%.\n",
      "Seed: 8, C: 5, H: 0.002, L: 3e-06\n",
      "Detection Acc: 91.1%, False Negative: 358, False Positive: 514, True Negative: 4433, True Positive: 4589.\n",
      "Computing final model predictions...\n",
      "Reclassification Acc: 91.7%, Clean Correct Acc: 89.9%, Clean Incorrect Acc: 0.0%, Adv Correct Acc: 1.86%, Adv Incorrect Acc: 8.21%.\n",
      "seed: 9\n",
      "Loading the data and model...\n",
      "(60000, 28, 28, 1)\n",
      "(60000, 10)\n",
      "(10000, 28, 28, 1)\n",
      "(10000, 10)\n",
      "Loading adversarial samples...\n",
      "Model accuracy on the normal test set: 99.00%\n",
      "Model accuracy on the adversarial test set: 19.02%\n",
      "Model accuracy on the combined undefended test set: 59.01%\n",
      "Getting Monte Carlo dropout variance predictions...\n",
      "Seed: 9, C: 5, H: 0.0005271433328744024, L: 4.13648049288895e-05\n",
      "Detection Acc: 92.0%, False Negative: 603, False Positive: 182, True Negative: 4768, True Positive: 4347.\n",
      "Computing final model predictions...\n",
      "Reclassification Acc: 96.1%, Clean Correct Acc: 95.9%, Clean Incorrect Acc: 0.0%, Adv Correct Acc: 0.13%, Adv Incorrect Acc: 3.88%.\n",
      "Seed: 9, C: 5, H: 0.002, L: 3e-06\n",
      "Detection Acc: 90.7%, False Negative: 367, False Positive: 546, True Negative: 4404, True Positive: 4583.\n",
      "Computing final model predictions...\n",
      "Reclassification Acc: 91.1%, Clean Correct Acc: 89.3%, Clean Incorrect Acc: 0.0%, Adv Correct Acc: 1.77%, Adv Incorrect Acc: 8.87%.\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "def get_mc_predictions(model, X, nb_iter=50, batch_size=256):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    :param model:\n",
    "    :param X:\n",
    "    :param nb_iter:\n",
    "    :param batch_size:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    output_dim = model.layers[-1].output.shape[-1].value\n",
    "    get_output = K.function(\n",
    "        [model.layers[0].input, K.learning_phase()],\n",
    "        [model.layers[-1].output]\n",
    "    )\n",
    "\n",
    "    def predict():\n",
    "        n_batches = int(np.ceil(X.shape[0] / float(batch_size)))\n",
    "        output = np.zeros(shape=(len(X), output_dim))\n",
    "        for i in range(n_batches):\n",
    "            output[i * batch_size:(i + 1) * batch_size] = \\\n",
    "                get_output([X[i * batch_size:(i + 1) * batch_size], 1])[0]\n",
    "        return output\n",
    "\n",
    "    preds_mc = []\n",
    "    for i in range(nb_iter):\n",
    "        preds_mc.append(predict())\n",
    "\n",
    "    return np.asarray(preds_mc)\n",
    "\n",
    "attack='fgsm'\n",
    "for sd in range(0,10):\n",
    "    print(\"seed: \"+str(sd))\n",
    "    with_norm=False\n",
    "    batch_size=256\n",
    "    #main(attack,with_norm,batch_size)\n",
    "    np.random.seed(sd)\n",
    "    idx0=np.random.choice(10000,5000)\n",
    "    dataset='mnist'\n",
    "    text_file = open(\"../stats/\"+attack+\"_stats.txt\", \"w\")\n",
    "    assert attack in ['fgsm', 'bim', 'bim-a', 'bim-b', 'jsma'], \\\n",
    "        \"Attack parameter must be either 'fgsm', 'bim', bim-a', 'bim-b', 'jsma'\"\n",
    "    assert os.path.isfile('../data/model_%s.h5' % dataset), \\\n",
    "        'model file not found... must first train model using train_model.py.'\n",
    "    assert os.path.isfile('../data/Adv_%s_%s.npy' % (dataset, attack)), \\\n",
    "        'adversarial sample file not found... must first craft adversarial ' \\\n",
    "        'samples using craft_adv_samples.py'\n",
    "    print('Loading the data and model...')\n",
    "    # Load the model\n",
    "    model = load_model('../data/model_%s.h5' % dataset)\n",
    "    # Load the dataset\n",
    "    X_train, Y_train, X_test, Y_test = get_data()\n",
    "    # Check attack type, select adversarial samples accordingly\n",
    "    print('Loading adversarial samples...')\n",
    "    X_test_adv = np.load('../data/Adv_%s_%s.npy' % (dataset, attack))\n",
    "\n",
    "\n",
    "\n",
    "    ################################################ Table 1 ###########################################\n",
    "    #Get half data from clean and adversarial images respectively and combine them\n",
    "    X_test, Y_test, X_test_adv = X_test[idx0], Y_test[idx0], X_test_adv[idx0]\n",
    "    X_test_all_un = np.concatenate((X_test, X_test_adv),axis=0)\n",
    "    Y_test_all_un = np.concatenate((Y_test, Y_test),axis=0)\n",
    "    # Check model accuracies on each sample type and then on combined undefended dataset\n",
    "    for s_type, dt in zip(['normal', 'adversarial'], [X_test, X_test_adv]):\n",
    "        _, acc = model.evaluate(dt, Y_test, batch_size=batch_size, verbose=0)\n",
    "        print(\"Model accuracy on the %s test set: %0.2f%%\" % (s_type, 100 * acc))\n",
    "    _, acc = model.evaluate(X_test_all_un, Y_test_all_un, batch_size=batch_size, verbose=0)\n",
    "    print(\"Model accuracy on the combined undefended test set: %0.2f%%\" % (100 * acc))\n",
    "    \n",
    "    \n",
    "    ################################################ Table 2 ###########################################\n",
    "    # Refine the normal and adversarial sets to only include samples,\n",
    "    # for which the original version was correctly classified by the model.\n",
    "    # Then, create detector label for clean as \"1\" and for adversarial as \"0\"\n",
    "    preds_test = model.predict_classes(X_test, verbose=0, batch_size=batch_size)\n",
    "    inds_correct = np.where(preds_test == Y_test.argmax(axis=1))[0]\n",
    "    X_test = X_test[inds_correct]\n",
    "    X_test_adv = X_test_adv[inds_correct]\n",
    "    Y_test = Y_test[inds_correct]\n",
    "    label_clean=np.ones(Y_test.shape[0])\n",
    "    label_adv=np.zeros(Y_test.shape[0])\n",
    "\n",
    "    # Combine the filtered dataset and detector label\n",
    "    X_test_all_filtered = np.concatenate((X_test, X_test_adv),axis=0)\n",
    "    Y_test_all_filtered = np.concatenate((Y_test, Y_test),axis=0)\n",
    "    label_filtered = np.concatenate((label_clean, label_adv),axis=0)\n",
    "\n",
    "    # Get Prediction + Bayesian uncertainty scores\n",
    "    print('Getting Monte Carlo dropout variance predictions...')\n",
    "    #pred_normal = get_mc_predictions(model, X_test, batch_size=batch_size)\n",
    "    #pred_adv = get_mc_predictions(model, X_test_adv, batch_size=batch_size)\n",
    "    pred_all_filtered = get_mc_predictions(model, X_test_all_filtered, batch_size=batch_size)\n",
    "    #uncerts_normal = pred_normal.var(axis=0).mean(axis=1)\n",
    "    #uncerts_adv = pred_adv.var(axis=0).mean(axis=1)\n",
    "    uncerts_all = pred_all_filtered.var(axis=0).mean(axis=1)\n",
    "    if with_norm:\n",
    "        ## Z-score the uncertainty\n",
    "        uncerts_all = normalize(uncerts_all)\n",
    "    #uncerts_all = np.concatenate((uncerts_normal, uncerts_adv),axis=0)\n",
    "    #class_normal = pred_normal.mean(axis=0).argmax(axis=1)\n",
    "    #class_adv = pred_adv.mean(axis=0).argmax(axis=1)\n",
    "    #preds_test_all_filtered = np.concatenate((class_normal, class_adv),axis=0)\n",
    "    preds_test_all_filtered = pred_all_filtered.mean(axis=0).argmax(axis=1)\n",
    "    \n",
    "    # Detector Parameters to be fine-tuned in experiments\n",
    "    params = {\n",
    "        'fgsm': {'H': 0.002,  'L': 0.000003, 'C': 5},\n",
    "        'bim':  {'H': 0.0022, 'L': 0.0012,   'C': 5},\n",
    "        'jsma': {'H': 0.01,   'L': 0.003,    'C': 5}\n",
    "    }\n",
    "    start_H, start_L, start_C = params[attack][\"H\"], params[attack][\"L\"], params[attack][\"C\"]\n",
    "    H_range, L_range, C_range = 0, 0, 1\n",
    "    H_step, L_step, C_step = 1e-4, 5e-6, 1\n",
    "\n",
    "    threshold_ls=[]\n",
    "    estimator = DecisionTreeClassifier(criterion='entropy',max_leaf_nodes=3, splitter='best',random_state=None)\n",
    "    X = np.expand_dims(uncerts_all,axis=-1)\n",
    "    Y = label_filtered\n",
    "    estimator.fit(X, Y)\n",
    "    n_nodes = estimator.tree_.node_count\n",
    "    children_left = estimator.tree_.children_left\n",
    "    children_right = estimator.tree_.children_right\n",
    "    threshold = estimator.tree_.threshold\n",
    "    node_depth = np.zeros(shape=n_nodes, dtype=np.int64)\n",
    "    is_leaves = np.zeros(shape=n_nodes, dtype=bool)\n",
    "    stack = [(0, -1)]  # seed is the root node id and its parent depth\n",
    "    while len(stack) > 0:\n",
    "        node_id, parent_depth = stack.pop()\n",
    "        node_depth[node_id] = parent_depth + 1\n",
    "        if (children_left[node_id] != children_right[node_id]):\n",
    "            stack.append((children_left[node_id], parent_depth + 1))\n",
    "            stack.append((children_right[node_id], parent_depth + 1))\n",
    "        else:\n",
    "            is_leaves[node_id] = True\n",
    "    for i in range(n_nodes):\n",
    "        if not is_leaves[i]:\n",
    "            threshold_ls.append(threshold[i])\n",
    "    start_H_d=max(threshold_ls)\n",
    "    start_L_d=min(threshold_ls)\n",
    "\n",
    "    #Detection\n",
    "    for C in range(start_C, min(start_C+C_range,5)+C_step, C_step):\n",
    "        for sad in range(2):\n",
    "            H=[start_H_d,start_H][sad]\n",
    "            L=[start_L_d,start_L][sad]\n",
    "            label_pred=detect_clean_adv(X_test_all_filtered, preds_test_all_filtered, uncerts_all, model, C, H, L, batch_size)\n",
    "            #Detection Evaluation\n",
    "            CM=confusion_matrix(label_filtered, label_pred)\n",
    "            TN = CM[0][0]\n",
    "            FN = CM[1][0]\n",
    "            TP = CM[1][1]\n",
    "            FP = CM[0][1]\n",
    "            ACC_DETECT = (TP+TN)/(TP+TN+FP+FN)\n",
    "            res_detect = \"Seed: \" + str(sd) + \", C: \" + str(C) + \", H: \" + str(H)+ \", L: \" + str(L) \\\n",
    "                            + \"\\nDetection Acc: \" + str(ACC_DETECT*100)[:4] \\\n",
    "                            + \"%, False Negative: \" + str(FN) + \", False Positive: \" + str(FP) \\\n",
    "                            + \", True Negative: \" + str(TN) + \", True Positive: \" + str(TP)+\".\"\n",
    "            print(res_detect)\n",
    "            text_file.write(res_detect+\"\\n\")\n",
    "\n",
    "            ################################################ Table 3 ###########################################\n",
    "            #Only get images reported as clean\n",
    "            idx_clean_reported=np.where(label_pred)[0]\n",
    "            X_test_all_def = X_test_all_filtered[idx_clean_reported]\n",
    "            Y_test_all_def = Y_test_all_filtered[idx_clean_reported]\n",
    "            label_def = label_filtered[idx_clean_reported]\n",
    "            num_all=label_def.shape[0]\n",
    "\n",
    "            # Reclassification\n",
    "            print('Computing final model predictions...')\n",
    "            preds_test_all_def = model.predict_classes(X_test_all_def, verbose=0, batch_size=batch_size)\n",
    "            inds_correct_def = np.where(preds_test_all_def == Y_test_all_def.argmax(axis=1))[0]\n",
    "            inds_incorrect_def = np.where(preds_test_all_def != Y_test_all_def.argmax(axis=1))[0]\n",
    "\n",
    "            # Reclassification Evaluation \n",
    "            num_clean_correct = np.argwhere(label_def[inds_correct_def] == 1).shape[0]\n",
    "            num_clean_incorrect = np.argwhere(label_def[inds_incorrect_def] == 1).shape[0]\n",
    "            num_adv_correct = np.argwhere(label_def[inds_correct_def] == 0).shape[0]\n",
    "            num_adv_incorrect = np.argwhere(label_def[inds_incorrect_def] == 0).shape[0]\n",
    "            clean_correct_acc = num_clean_correct/num_all\n",
    "            clean_incorrect_acc = num_clean_incorrect/num_all\n",
    "            adv_correct_acc = num_adv_correct/num_all\n",
    "            adv_incorrect_acc = num_adv_incorrect/num_all\n",
    "            total_acc = clean_correct_acc + adv_correct_acc\n",
    "            res_reclf = \"Reclassification Acc: \" + str(total_acc*100)[:4] \\\n",
    "            + \"%, Clean Correct Acc: \" + str(clean_correct_acc*100)[:4] \\\n",
    "            + \"%, Clean Incorrect Acc: \" + str(clean_incorrect_acc*100)[:4] \\\n",
    "            + \"%, Adv Correct Acc: \" + str(adv_correct_acc*100)[:4] \\\n",
    "            + \"%, Adv Incorrect Acc: \" + str(adv_incorrect_acc*100)[:4] + \"%.\"\n",
    "            print(res_reclf)\n",
    "            text_file.write(res_reclf+\"\\n\")\n",
    "\n",
    "\n",
    "    text_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C: 5, H: 0.008, L: 3e-06\n",
      "Detection Acc: 75.7%, False Negative: 175, False Positive: 2223, True Negative: 2728, True Positive: 4776.\n",
      "Computing final model predictions...\n",
      "Reclassification Acc: 73.3%, Clean Correct Acc: 68.2%, Clean Incorrect Acc: 0.0%, Adv Correct Acc: 5.07%, Adv Incorrect Acc: 26.6%.\n"
     ]
    }
   ],
   "source": [
    "H=0.008\n",
    "L=start_L\n",
    "label_pred=detect_clean_adv(X_test_all_filtered, preds_test_all_filtered, uncerts_all, model, C, H, L, batch_size)\n",
    "#Detection Evaluation\n",
    "CM=confusion_matrix(label_filtered, label_pred)\n",
    "TN = CM[0][0]\n",
    "FN = CM[1][0]\n",
    "TP = CM[1][1]\n",
    "FP = CM[0][1]\n",
    "ACC_DETECT = (TP+TN)/(TP+TN+FP+FN)\n",
    "res_detect = \"Seed: \" + str(sd) + \", C: \" + str(C) + \", H: \" + str(H)+ \", L: \" + str(L) \\\n",
    "                + \"\\nDetection Acc: \" + str(ACC_DETECT*100)[:4] \\\n",
    "                + \"%, False Negative: \" + str(FN) + \", False Positive: \" + str(FP) \\\n",
    "                + \", True Negative: \" + str(TN) + \", True Positive: \" + str(TP)+\".\"\n",
    "print(res_detect)\n",
    "\n",
    "################################################ Table 3 ###########################################\n",
    "#Only get images reported as clean\n",
    "idx_clean_reported=np.where(label_pred)[0]\n",
    "X_test_all_def = X_test_all_filtered[idx_clean_reported]\n",
    "Y_test_all_def = Y_test_all_filtered[idx_clean_reported]\n",
    "label_def = label_filtered[idx_clean_reported]\n",
    "num_all=label_def.shape[0]\n",
    "\n",
    "# Reclassification\n",
    "print('Computing final model predictions...')\n",
    "preds_test_all_def = model.predict_classes(X_test_all_def, verbose=0, batch_size=batch_size)\n",
    "inds_correct_def = np.where(preds_test_all_def == Y_test_all_def.argmax(axis=1))[0]\n",
    "inds_incorrect_def = np.where(preds_test_all_def != Y_test_all_def.argmax(axis=1))[0]\n",
    "\n",
    "# Reclassification Evaluation \n",
    "num_clean_correct = np.argwhere(label_def[inds_correct_def] == 1).shape[0]\n",
    "num_clean_incorrect = np.argwhere(label_def[inds_incorrect_def] == 1).shape[0]\n",
    "num_adv_correct = np.argwhere(label_def[inds_correct_def] == 0).shape[0]\n",
    "num_adv_incorrect = np.argwhere(label_def[inds_incorrect_def] == 0).shape[0]\n",
    "clean_correct_acc = num_clean_correct/num_all\n",
    "clean_incorrect_acc = num_clean_incorrect/num_all\n",
    "adv_correct_acc = num_adv_correct/num_all\n",
    "adv_incorrect_acc = num_adv_incorrect/num_all\n",
    "total_acc = clean_correct_acc + adv_correct_acc\n",
    "res_reclf = \"Reclassification Acc: \" + str(total_acc*100)[:4] \\\n",
    "+ \"%, Clean Correct Acc: \" + str(clean_correct_acc*100)[:4] \\\n",
    "+ \"%, Clean Incorrect Acc: \" + str(clean_incorrect_acc*100)[:4] \\\n",
    "+ \"%, Adv Correct Acc: \" + str(adv_correct_acc*100)[:4] \\\n",
    "+ \"%, Adv Incorrect Acc: \" + str(adv_incorrect_acc*100)[:4] + \"%.\"\n",
    "print(res_reclf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "id=1\n",
    "a=X_test[id]\n",
    "print(Y_test[id].argmax(axis=-1))\n",
    "a=np.squeeze(a,axis=-1)\n",
    "plt.axis('off')\n",
    "plt.imshow(a, cmap='gray')\n",
    "plt.show()\n",
    "X_test_adv = np.load('../data/Adv_%s_%s.npy' % ('mnist', 'bim'))\n",
    "b=X_test_adv[id:id+1]\n",
    "model = load_model('../data/model_%s.h5' % 'mnist')\n",
    "print(model.predict_classes(b, verbose=0))\n",
    "b=np.squeeze(b)\n",
    "plt.axis('off')\n",
    "plt.imshow(b, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(\"seed: \"+str(sd))\\nattack=\\'fgsm\\'\\nwith_norm=False\\nbatch_size=256\\n#main(attack,with_norm,batch_size)\\nnp.random.seed(sd)\\nidx0=np.random.choice(10000,5000)\\ndataset=\\'mnist\\'\\ntext_file = open(\"../stats/\"+attack+\"_stats.txt\", \"w\")\\nassert attack in [\\'fgsm\\', \\'bim\\', \\'bim-a\\', \\'bim-b\\', \\'jsma\\'],     \"Attack parameter must be either \\'fgsm\\', \\'bim\\', bim-a\\', \\'bim-b\\', \\'jsma\\'\"\\nassert os.path.isfile(\\'../data/model_%s.h5\\' % dataset),     \\'model file not found... must first train model using train_model.py.\\'\\nassert os.path.isfile(\\'../data/Adv_%s_%s.npy\\' % (dataset, attack)),     \\'adversarial sample file not found... must first craft adversarial \\'     \\'samples using craft_adv_samples.py\\'\\nprint(\\'Loading the data and model...\\')\\n# Load the model\\nmodel = load_model(\\'../data/model_%s.h5\\' % dataset)\\n# Load the dataset\\nX_train, Y_train, X_test, Y_test = get_data()\\n# Check attack type, select adversarial samples accordingly\\nprint(\\'Loading adversarial samples...\\')\\nX_test_adv = np.load(\\'../data/Adv_%s_%s.npy\\' % (dataset, attack))\\n\\n\\n\\n################################################ Table 1 ###########################################\\n#Get half data from clean and adversarial images respectively and combine them\\nX_test, Y_test, X_test_adv = X_test[idx0], Y_test[idx0], X_test_adv[idx0]\\nX_test_all_un = np.concatenate((X_test, X_test_adv),axis=0)\\nY_test_all_un = np.concatenate((Y_test, Y_test),axis=0)\\n# Check model accuracies on each sample type and then on combined undefended dataset\\nfor s_type, dt in zip([\\'normal\\', \\'adversarial\\'], [X_test, X_test_adv]):\\n    _, acc = model.evaluate(dt, Y_test, batch_size=batch_size, verbose=0)\\n    print(\"Model accuracy on the %s test set: %0.2f%%\" % (s_type, 100 * acc))\\n_, acc = model.evaluate(X_test_all_un, Y_test_all_un, batch_size=batch_size, verbose=0)\\nprint(\"Model accuracy on the combined undefended test set: %0.2f%%\" % (100 * acc))\\n\\n\\n################################################ Table 2 ###########################################\\n# Refine the normal and adversarial sets to only include samples,\\n# for which the original version was correctly classified by the model.\\n# Then, create detector label for clean as \"1\" and for adversarial as \"0\"\\npreds_test = model.predict_classes(X_test, verbose=0, batch_size=batch_size)\\ninds_correct = np.where(preds_test == Y_test.argmax(axis=1))[0]\\nX_test = X_test[inds_correct]\\nX_test_adv = X_test_adv[inds_correct]\\nY_test = Y_test[inds_correct]\\nlabel_clean=np.ones(Y_test.shape[0])\\nlabel_adv=np.zeros(Y_test.shape[0])\\n\\n# Combine the filtered dataset and detector label\\nX_test_all_filtered = np.concatenate((X_test, X_test_adv),axis=0)\\nY_test_all_filtered = np.concatenate((Y_test, Y_test),axis=0)\\nlabel_filtered = np.concatenate((label_clean, label_adv),axis=0)\\n\\n# Get Prediction + Bayesian uncertainty scores\\nprint(\\'Getting Monte Carlo dropout variance predictions...\\')\\n#pred_normal = get_mc_predictions(model, X_test, batch_size=batch_size)\\n#pred_adv = get_mc_predictions(model, X_test_adv, batch_size=batch_size)\\npred_all_filtered = get_mc_predictions(model, X_test_all_filtered, batch_size=batch_size)\\n#uncerts_normal = pred_normal.var(axis=0).mean(axis=1)\\n#uncerts_adv = pred_adv.var(axis=0).mean(axis=1)\\nuncerts_all = pred_all_filtered.var(axis=0).mean(axis=1)\\nif with_norm:\\n    ## Z-score the uncertainty\\n    uncerts_all = normalize(uncerts_all)\\n#uncerts_all = np.concatenate((uncerts_normal, uncerts_adv),axis=0)\\n#class_normal = pred_normal.mean(axis=0).argmax(axis=1)\\n#class_adv = pred_adv.mean(axis=0).argmax(axis=1)\\n#preds_test_all_filtered = np.concatenate((class_normal, class_adv),axis=0)\\npreds_test_all_filtered = pred_all_filtered.mean(axis=0).argmax(axis=1)\\n\\n# Detector Parameters to be fine-tuned in experiments\\nparams = {\\n    \\'fgsm\\': {\\'H\\': 0.0007512824086006731,  \\'L\\': 2.7283624149276875e-05, \\'C\\': 5},\\n    \\'bim\\':  {\\'H\\': 0.0022, \\'L\\': 0.0012,   \\'C\\': 5},\\n    \\'jsma\\': {\\'H\\': 0.01,   \\'L\\': 0.003,    \\'C\\': 4}\\n}\\nstart_H, start_L, start_C = params[attack][\"H\"], params[attack][\"L\"], params[attack][\"C\"]\\nH_range, L_range, C_range = 0, 0, 2\\nH_step, L_step, C_step = 1e-4, 5e-6, 1\\n\\nthreshold_ls=[]\\nestimator = DecisionTreeClassifier(criterion=\\'entropy\\',max_leaf_nodes=3, splitter=\\'best\\',random_state=None)\\nX = np.expand_dims(uncerts_all,axis=-1)\\nY = label_filtered\\nestimator.fit(X, Y)\\nn_nodes = estimator.tree_.node_count\\nchildren_left = estimator.tree_.children_left\\nchildren_right = estimator.tree_.children_right\\nthreshold = estimator.tree_.threshold\\nnode_depth = np.zeros(shape=n_nodes, dtype=np.int64)\\nis_leaves = np.zeros(shape=n_nodes, dtype=bool)\\nstack = [(0, -1)]  # seed is the root node id and its parent depth\\nwhile len(stack) > 0:\\n    node_id, parent_depth = stack.pop()\\n    node_depth[node_id] = parent_depth + 1\\n    if (children_left[node_id] != children_right[node_id]):\\n        stack.append((children_left[node_id], parent_depth + 1))\\n        stack.append((children_right[node_id], parent_depth + 1))\\n    else:\\n        is_leaves[node_id] = True\\nfor i in range(n_nodes):\\n    if not is_leaves[i]:\\n        threshold_ls.append(threshold[i])\\n#start_H=max(threshold_ls)\\n#start_L=min(threshold_ls)\\n\\n#Detection\\nfor C in range(start_C, min(start_C+C_range,5)+C_step, C_step):\\n    for H in np.arange(start_H, start_H+H_range+H_step-1e-10, H_step):\\n        for L in np.arange(start_L, start_L+L_range+L_step-1e-10, L_step):\\n            label_pred=detect_clean_adv(X_test_all_filtered, preds_test_all_filtered, uncerts_all, model, C, H, L, batch_size)\\n            #Detection Evaluation\\n            CM=confusion_matrix(label_filtered, label_pred)\\n            TN = CM[0][0]\\n            FN = CM[1][0]\\n            TP = CM[1][1]\\n            FP = CM[0][1]\\n            ACC_DETECT = (TP+TN)/(TP+TN+FP+FN)\\n            res_detect = \"C: \" + str(C) + \", H: \" + str(H)+ \", L: \" + str(L)                             + \"\\nDetection Acc: \" + str(ACC_DETECT*100)[:4]                             + \"%, False Negative: \" + str(FN) + \", False Positive: \" + str(FP)                             + \", True Negative: \" + str(TN) + \", True Positive: \" + str(TP)+\".\"\\n            print(res_detect)\\n            text_file.write(res_detect+\"\\n\")\\n            \\n            \\n            \\n            ################################################ Table 3 ###########################################\\n            #Only get images reported as clean\\n            idx_clean_reported=np.where(label_pred)[0]\\n            X_test_all_def = X_test_all_filtered[idx_clean_reported]\\n            Y_test_all_def = Y_test_all_filtered[idx_clean_reported]\\n            label_def = label_filtered[idx_clean_reported]\\n            num_all=label_def.shape[0]\\n\\n            # Reclassification\\n            print(\\'Computing final model predictions...\\')\\n            preds_test_all_def = model.predict_classes(X_test_all_def, verbose=0, batch_size=batch_size)\\n            inds_correct_def = np.where(preds_test_all_def == Y_test_all_def.argmax(axis=1))[0]\\n            inds_incorrect_def = np.where(preds_test_all_def != Y_test_all_def.argmax(axis=1))[0]\\n\\n            # Reclassification Evaluation \\n            num_clean_correct = np.argwhere(label_def[inds_correct_def] == 1).shape[0]\\n            num_clean_incorrect = np.argwhere(label_def[inds_incorrect_def] == 1).shape[0]\\n            num_adv_correct = np.argwhere(label_def[inds_correct_def] == 0).shape[0]\\n            num_adv_incorrect = np.argwhere(label_def[inds_incorrect_def] == 0).shape[0]\\n            clean_correct_acc = num_clean_correct/num_all\\n            clean_incorrect_acc = num_clean_incorrect/num_all\\n            adv_correct_acc = num_adv_correct/num_all\\n            adv_incorrect_acc = num_adv_incorrect/num_all\\n            total_acc = clean_correct_acc + adv_correct_acc\\n            res_reclf = \"Reclassification Acc: \" + str(total_acc*100)[:4]             + \"%, Clean Correct Acc: \" + str(clean_correct_acc*100)[:4]             + \"%, Clean Incorrect Acc: \" + str(clean_incorrect_acc*100)[:4]             + \"%, Adv Correct Acc: \" + str(adv_correct_acc*100)[:4]             + \"%, Adv Incorrect Acc: \" + str(adv_incorrect_acc*100)[:4] + \"%.\"\\n            print(res_reclf)\\n            text_file.write(res_reclf+\"\\n\")\\n\\n\\ntext_file.close()\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "print(\"seed: \"+str(sd))\n",
    "attack='fgsm'\n",
    "with_norm=False\n",
    "batch_size=256\n",
    "#main(attack,with_norm,batch_size)\n",
    "np.random.seed(sd)\n",
    "idx0=np.random.choice(10000,5000)\n",
    "dataset='mnist'\n",
    "text_file = open(\"../stats/\"+attack+\"_stats.txt\", \"w\")\n",
    "assert attack in ['fgsm', 'bim', 'bim-a', 'bim-b', 'jsma'], \\\n",
    "    \"Attack parameter must be either 'fgsm', 'bim', bim-a', 'bim-b', 'jsma'\"\n",
    "assert os.path.isfile('../data/model_%s.h5' % dataset), \\\n",
    "    'model file not found... must first train model using train_model.py.'\n",
    "assert os.path.isfile('../data/Adv_%s_%s.npy' % (dataset, attack)), \\\n",
    "    'adversarial sample file not found... must first craft adversarial ' \\\n",
    "    'samples using craft_adv_samples.py'\n",
    "print('Loading the data and model...')\n",
    "# Load the model\n",
    "model = load_model('../data/model_%s.h5' % dataset)\n",
    "# Load the dataset\n",
    "X_train, Y_train, X_test, Y_test = get_data()\n",
    "# Check attack type, select adversarial samples accordingly\n",
    "print('Loading adversarial samples...')\n",
    "X_test_adv = np.load('../data/Adv_%s_%s.npy' % (dataset, attack))\n",
    "\n",
    "\n",
    "\n",
    "################################################ Table 1 ###########################################\n",
    "#Get half data from clean and adversarial images respectively and combine them\n",
    "X_test, Y_test, X_test_adv = X_test[idx0], Y_test[idx0], X_test_adv[idx0]\n",
    "X_test_all_un = np.concatenate((X_test, X_test_adv),axis=0)\n",
    "Y_test_all_un = np.concatenate((Y_test, Y_test),axis=0)\n",
    "# Check model accuracies on each sample type and then on combined undefended dataset\n",
    "for s_type, dt in zip(['normal', 'adversarial'], [X_test, X_test_adv]):\n",
    "    _, acc = model.evaluate(dt, Y_test, batch_size=batch_size, verbose=0)\n",
    "    print(\"Model accuracy on the %s test set: %0.2f%%\" % (s_type, 100 * acc))\n",
    "_, acc = model.evaluate(X_test_all_un, Y_test_all_un, batch_size=batch_size, verbose=0)\n",
    "print(\"Model accuracy on the combined undefended test set: %0.2f%%\" % (100 * acc))\n",
    "\n",
    "\n",
    "################################################ Table 2 ###########################################\n",
    "# Refine the normal and adversarial sets to only include samples,\n",
    "# for which the original version was correctly classified by the model.\n",
    "# Then, create detector label for clean as \"1\" and for adversarial as \"0\"\n",
    "preds_test = model.predict_classes(X_test, verbose=0, batch_size=batch_size)\n",
    "inds_correct = np.where(preds_test == Y_test.argmax(axis=1))[0]\n",
    "X_test = X_test[inds_correct]\n",
    "X_test_adv = X_test_adv[inds_correct]\n",
    "Y_test = Y_test[inds_correct]\n",
    "label_clean=np.ones(Y_test.shape[0])\n",
    "label_adv=np.zeros(Y_test.shape[0])\n",
    "\n",
    "# Combine the filtered dataset and detector label\n",
    "X_test_all_filtered = np.concatenate((X_test, X_test_adv),axis=0)\n",
    "Y_test_all_filtered = np.concatenate((Y_test, Y_test),axis=0)\n",
    "label_filtered = np.concatenate((label_clean, label_adv),axis=0)\n",
    "\n",
    "# Get Prediction + Bayesian uncertainty scores\n",
    "print('Getting Monte Carlo dropout variance predictions...')\n",
    "#pred_normal = get_mc_predictions(model, X_test, batch_size=batch_size)\n",
    "#pred_adv = get_mc_predictions(model, X_test_adv, batch_size=batch_size)\n",
    "pred_all_filtered = get_mc_predictions(model, X_test_all_filtered, batch_size=batch_size)\n",
    "#uncerts_normal = pred_normal.var(axis=0).mean(axis=1)\n",
    "#uncerts_adv = pred_adv.var(axis=0).mean(axis=1)\n",
    "uncerts_all = pred_all_filtered.var(axis=0).mean(axis=1)\n",
    "if with_norm:\n",
    "    ## Z-score the uncertainty\n",
    "    uncerts_all = normalize(uncerts_all)\n",
    "#uncerts_all = np.concatenate((uncerts_normal, uncerts_adv),axis=0)\n",
    "#class_normal = pred_normal.mean(axis=0).argmax(axis=1)\n",
    "#class_adv = pred_adv.mean(axis=0).argmax(axis=1)\n",
    "#preds_test_all_filtered = np.concatenate((class_normal, class_adv),axis=0)\n",
    "preds_test_all_filtered = pred_all_filtered.mean(axis=0).argmax(axis=1)\n",
    "\n",
    "# Detector Parameters to be fine-tuned in experiments\n",
    "params = {\n",
    "    'fgsm': {'H': 0.0007512824086006731,  'L': 2.7283624149276875e-05, 'C': 5},\n",
    "    'bim':  {'H': 0.0022, 'L': 0.0012,   'C': 5},\n",
    "    'jsma': {'H': 0.01,   'L': 0.003,    'C': 4}\n",
    "}\n",
    "start_H, start_L, start_C = params[attack][\"H\"], params[attack][\"L\"], params[attack][\"C\"]\n",
    "H_range, L_range, C_range = 0, 0, 2\n",
    "H_step, L_step, C_step = 1e-4, 5e-6, 1\n",
    "\n",
    "threshold_ls=[]\n",
    "estimator = DecisionTreeClassifier(criterion='entropy',max_leaf_nodes=3, splitter='best',random_state=None)\n",
    "X = np.expand_dims(uncerts_all,axis=-1)\n",
    "Y = label_filtered\n",
    "estimator.fit(X, Y)\n",
    "n_nodes = estimator.tree_.node_count\n",
    "children_left = estimator.tree_.children_left\n",
    "children_right = estimator.tree_.children_right\n",
    "threshold = estimator.tree_.threshold\n",
    "node_depth = np.zeros(shape=n_nodes, dtype=np.int64)\n",
    "is_leaves = np.zeros(shape=n_nodes, dtype=bool)\n",
    "stack = [(0, -1)]  # seed is the root node id and its parent depth\n",
    "while len(stack) > 0:\n",
    "    node_id, parent_depth = stack.pop()\n",
    "    node_depth[node_id] = parent_depth + 1\n",
    "    if (children_left[node_id] != children_right[node_id]):\n",
    "        stack.append((children_left[node_id], parent_depth + 1))\n",
    "        stack.append((children_right[node_id], parent_depth + 1))\n",
    "    else:\n",
    "        is_leaves[node_id] = True\n",
    "for i in range(n_nodes):\n",
    "    if not is_leaves[i]:\n",
    "        threshold_ls.append(threshold[i])\n",
    "#start_H=max(threshold_ls)\n",
    "#start_L=min(threshold_ls)\n",
    "\n",
    "#Detection\n",
    "for C in range(start_C, min(start_C+C_range,5)+C_step, C_step):\n",
    "    for H in np.arange(start_H, start_H+H_range+H_step-1e-10, H_step):\n",
    "        for L in np.arange(start_L, start_L+L_range+L_step-1e-10, L_step):\n",
    "            label_pred=detect_clean_adv(X_test_all_filtered, preds_test_all_filtered, uncerts_all, model, C, H, L, batch_size)\n",
    "            #Detection Evaluation\n",
    "            CM=confusion_matrix(label_filtered, label_pred)\n",
    "            TN = CM[0][0]\n",
    "            FN = CM[1][0]\n",
    "            TP = CM[1][1]\n",
    "            FP = CM[0][1]\n",
    "            ACC_DETECT = (TP+TN)/(TP+TN+FP+FN)\n",
    "            res_detect = \"C: \" + str(C) + \", H: \" + str(H)+ \", L: \" + str(L) \\\n",
    "                            + \"\\nDetection Acc: \" + str(ACC_DETECT*100)[:4] \\\n",
    "                            + \"%, False Negative: \" + str(FN) + \", False Positive: \" + str(FP) \\\n",
    "                            + \", True Negative: \" + str(TN) + \", True Positive: \" + str(TP)+\".\"\n",
    "            print(res_detect)\n",
    "            text_file.write(res_detect+\"\\n\")\n",
    "            \n",
    "            \n",
    "            \n",
    "            ################################################ Table 3 ###########################################\n",
    "            #Only get images reported as clean\n",
    "            idx_clean_reported=np.where(label_pred)[0]\n",
    "            X_test_all_def = X_test_all_filtered[idx_clean_reported]\n",
    "            Y_test_all_def = Y_test_all_filtered[idx_clean_reported]\n",
    "            label_def = label_filtered[idx_clean_reported]\n",
    "            num_all=label_def.shape[0]\n",
    "\n",
    "            # Reclassification\n",
    "            print('Computing final model predictions...')\n",
    "            preds_test_all_def = model.predict_classes(X_test_all_def, verbose=0, batch_size=batch_size)\n",
    "            inds_correct_def = np.where(preds_test_all_def == Y_test_all_def.argmax(axis=1))[0]\n",
    "            inds_incorrect_def = np.where(preds_test_all_def != Y_test_all_def.argmax(axis=1))[0]\n",
    "\n",
    "            # Reclassification Evaluation \n",
    "            num_clean_correct = np.argwhere(label_def[inds_correct_def] == 1).shape[0]\n",
    "            num_clean_incorrect = np.argwhere(label_def[inds_incorrect_def] == 1).shape[0]\n",
    "            num_adv_correct = np.argwhere(label_def[inds_correct_def] == 0).shape[0]\n",
    "            num_adv_incorrect = np.argwhere(label_def[inds_incorrect_def] == 0).shape[0]\n",
    "            clean_correct_acc = num_clean_correct/num_all\n",
    "            clean_incorrect_acc = num_clean_incorrect/num_all\n",
    "            adv_correct_acc = num_adv_correct/num_all\n",
    "            adv_incorrect_acc = num_adv_incorrect/num_all\n",
    "            total_acc = clean_correct_acc + adv_correct_acc\n",
    "            res_reclf = \"Reclassification Acc: \" + str(total_acc*100)[:4] \\\n",
    "            + \"%, Clean Correct Acc: \" + str(clean_correct_acc*100)[:4] \\\n",
    "            + \"%, Clean Incorrect Acc: \" + str(clean_incorrect_acc*100)[:4] \\\n",
    "            + \"%, Adv Correct Acc: \" + str(adv_correct_acc*100)[:4] \\\n",
    "            + \"%, Adv Incorrect Acc: \" + str(adv_incorrect_acc*100)[:4] + \"%.\"\n",
    "            print(res_reclf)\n",
    "            text_file.write(res_reclf+\"\\n\")\n",
    "\n",
    "\n",
    "text_file.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.tree import DecisionTreeClassifier\\n\\nestimator = DecisionTreeClassifier(criterion=\\'entropy\\',max_leaf_nodes=3, splitter=\\'best\\',random_state=None)\\nX = np.expand_dims(uncerts_all,axis=-1)\\nY = label_filtered\\nestimator.fit(X, Y)\\n# The decision estimator has an attribute called tree_  which stores the entire\\n# tree structure and allows access to low level attributes. The binary tree\\n# tree_ is represented as a number of parallel arrays. The i-th element of each\\n# array holds information about the node `i`. Node 0 is the tree\\'s root. NOTE:\\n# Some of the arrays only apply to either leaves or split nodes, resp. In this\\n# case the values of nodes of the other type are arbitrary!\\n#\\n# Among those arrays, we have:\\n#   - left_child, id of the left child of the node\\n#   - right_child, id of the right child of the node\\n#   - feature, feature used for splitting the node\\n#   - threshold, threshold value at the node\\n#\\n\\n# Using those arrays, we can parse the tree structure:\\n\\nn_nodes = estimator.tree_.node_count\\nchildren_left = estimator.tree_.children_left\\nchildren_right = estimator.tree_.children_right\\nfeature = estimator.tree_.feature\\nthreshold = estimator.tree_.threshold\\n\\n\\n# The tree structure can be traversed to compute various properties such\\n# as the depth of each node and whether or not it is a leaf.\\nnode_depth = np.zeros(shape=n_nodes, dtype=np.int64)\\nis_leaves = np.zeros(shape=n_nodes, dtype=bool)\\nstack = [(0, -1)]  # seed is the root node id and its parent depth\\nwhile len(stack) > 0:\\n    node_id, parent_depth = stack.pop()\\n    node_depth[node_id] = parent_depth + 1\\n\\n    # If we have a test node\\n    if (children_left[node_id] != children_right[node_id]):\\n        stack.append((children_left[node_id], parent_depth + 1))\\n        stack.append((children_right[node_id], parent_depth + 1))\\n    else:\\n        is_leaves[node_id] = True\\n\\nprint(\"The binary tree structure has %s nodes and has \"\\n      \"the following tree structure:\"\\n      % n_nodes)\\nfor i in range(n_nodes):\\n    if is_leaves[i]:\\n        print(\"%snode=%s leaf node.\" % (node_depth[i] * \"\\t\", i))\\n    else:\\n        print(\"%snode=%s test node: go to node %s if X[:, %s] <= %s else to \"\\n              \"node %s.\"\\n              % (node_depth[i] * \"\\t\",\\n                 i,\\n                 children_left[i],\\n                 feature[i],\\n                 threshold[i],\\n                 children_right[i],\\n                 ))\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "estimator = DecisionTreeClassifier(criterion='entropy',max_leaf_nodes=3, splitter='best',random_state=None)\n",
    "X = np.expand_dims(uncerts_all,axis=-1)\n",
    "Y = label_filtered\n",
    "estimator.fit(X, Y)\n",
    "# The decision estimator has an attribute called tree_  which stores the entire\n",
    "# tree structure and allows access to low level attributes. The binary tree\n",
    "# tree_ is represented as a number of parallel arrays. The i-th element of each\n",
    "# array holds information about the node `i`. Node 0 is the tree's root. NOTE:\n",
    "# Some of the arrays only apply to either leaves or split nodes, resp. In this\n",
    "# case the values of nodes of the other type are arbitrary!\n",
    "#\n",
    "# Among those arrays, we have:\n",
    "#   - left_child, id of the left child of the node\n",
    "#   - right_child, id of the right child of the node\n",
    "#   - feature, feature used for splitting the node\n",
    "#   - threshold, threshold value at the node\n",
    "#\n",
    "\n",
    "# Using those arrays, we can parse the tree structure:\n",
    "\n",
    "n_nodes = estimator.tree_.node_count\n",
    "children_left = estimator.tree_.children_left\n",
    "children_right = estimator.tree_.children_right\n",
    "feature = estimator.tree_.feature\n",
    "threshold = estimator.tree_.threshold\n",
    "\n",
    "\n",
    "# The tree structure can be traversed to compute various properties such\n",
    "# as the depth of each node and whether or not it is a leaf.\n",
    "node_depth = np.zeros(shape=n_nodes, dtype=np.int64)\n",
    "is_leaves = np.zeros(shape=n_nodes, dtype=bool)\n",
    "stack = [(0, -1)]  # seed is the root node id and its parent depth\n",
    "while len(stack) > 0:\n",
    "    node_id, parent_depth = stack.pop()\n",
    "    node_depth[node_id] = parent_depth + 1\n",
    "\n",
    "    # If we have a test node\n",
    "    if (children_left[node_id] != children_right[node_id]):\n",
    "        stack.append((children_left[node_id], parent_depth + 1))\n",
    "        stack.append((children_right[node_id], parent_depth + 1))\n",
    "    else:\n",
    "        is_leaves[node_id] = True\n",
    "\n",
    "print(\"The binary tree structure has %s nodes and has \"\n",
    "      \"the following tree structure:\"\n",
    "      % n_nodes)\n",
    "for i in range(n_nodes):\n",
    "    if is_leaves[i]:\n",
    "        print(\"%snode=%s leaf node.\" % (node_depth[i] * \"\\t\", i))\n",
    "    else:\n",
    "        print(\"%snode=%s test node: go to node %s if X[:, %s] <= %s else to \"\n",
    "              \"node %s.\"\n",
    "              % (node_depth[i] * \"\\t\",\n",
    "                 i,\n",
    "                 children_left[i],\n",
    "                 feature[i],\n",
    "                 threshold[i],\n",
    "                 children_right[i],\n",
    "                 ))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.tree import DecisionTreeClassifier\\n\\niris = load_iris()\\nX = iris.data\\ny = iris.target\\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\\n\\nestimator = DecisionTreeClassifier(criterion=\\'entropy\\',max_leaf_nodes=3, splitter=\\'best\\',random_state=None)\\nestimator.fit(X_train, y_train)\\nX = [[0], [0.5]]\\nY = [0, 1]\\nestimator.fit(X, Y)\\n# The decision estimator has an attribute called tree_  which stores the entire\\n# tree structure and allows access to low level attributes. The binary tree\\n# tree_ is represented as a number of parallel arrays. The i-th element of each\\n# array holds information about the node `i`. Node 0 is the tree\\'s root. NOTE:\\n# Some of the arrays only apply to either leaves or split nodes, resp. In this\\n# case the values of nodes of the other type are arbitrary!\\n#\\n# Among those arrays, we have:\\n#   - left_child, id of the left child of the node\\n#   - right_child, id of the right child of the node\\n#   - feature, feature used for splitting the node\\n#   - threshold, threshold value at the node\\n#\\n\\n# Using those arrays, we can parse the tree structure:\\n\\nn_nodes = estimator.tree_.node_count\\nchildren_left = estimator.tree_.children_left\\nchildren_right = estimator.tree_.children_right\\nfeature = estimator.tree_.feature\\nthreshold = estimator.tree_.threshold\\n\\n\\n# The tree structure can be traversed to compute various properties such\\n# as the depth of each node and whether or not it is a leaf.\\nnode_depth = np.zeros(shape=n_nodes, dtype=np.int64)\\nis_leaves = np.zeros(shape=n_nodes, dtype=bool)\\nstack = [(0, -1)]  # seed is the root node id and its parent depth\\nwhile len(stack) > 0:\\n    node_id, parent_depth = stack.pop()\\n    node_depth[node_id] = parent_depth + 1\\n\\n    # If we have a test node\\n    if (children_left[node_id] != children_right[node_id]):\\n        stack.append((children_left[node_id], parent_depth + 1))\\n        stack.append((children_right[node_id], parent_depth + 1))\\n    else:\\n        is_leaves[node_id] = True\\n\\nprint(\"The binary tree structure has %s nodes and has \"\\n      \"the following tree structure:\"\\n      % n_nodes)\\nfor i in range(n_nodes):\\n    if is_leaves[i]:\\n        print(\"%snode=%s leaf node.\" % (node_depth[i] * \"\\t\", i))\\n    else:\\n        print(\"%snode=%s test node: go to node %s if X[:, %s] <= %s else to \"\\n              \"node %s.\"\\n              % (node_depth[i] * \"\\t\",\\n                 i,\\n                 children_left[i],\\n                 feature[i],\\n                 threshold[i],\\n                 children_right[i],\\n                 ))\\nprint()\\n\\n# First let\\'s retrieve the decision path of each sample. The decision_path\\n# method allows to retrieve the node indicator functions. A non zero element of\\n# indicator matrix at the position (i, j) indicates that the sample i goes\\n# through the node j.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "estimator = DecisionTreeClassifier(criterion='entropy',max_leaf_nodes=3, splitter='best',random_state=None)\n",
    "estimator.fit(X_train, y_train)\n",
    "X = [[0], [0.5]]\n",
    "Y = [0, 1]\n",
    "estimator.fit(X, Y)\n",
    "# The decision estimator has an attribute called tree_  which stores the entire\n",
    "# tree structure and allows access to low level attributes. The binary tree\n",
    "# tree_ is represented as a number of parallel arrays. The i-th element of each\n",
    "# array holds information about the node `i`. Node 0 is the tree's root. NOTE:\n",
    "# Some of the arrays only apply to either leaves or split nodes, resp. In this\n",
    "# case the values of nodes of the other type are arbitrary!\n",
    "#\n",
    "# Among those arrays, we have:\n",
    "#   - left_child, id of the left child of the node\n",
    "#   - right_child, id of the right child of the node\n",
    "#   - feature, feature used for splitting the node\n",
    "#   - threshold, threshold value at the node\n",
    "#\n",
    "\n",
    "# Using those arrays, we can parse the tree structure:\n",
    "\n",
    "n_nodes = estimator.tree_.node_count\n",
    "children_left = estimator.tree_.children_left\n",
    "children_right = estimator.tree_.children_right\n",
    "feature = estimator.tree_.feature\n",
    "threshold = estimator.tree_.threshold\n",
    "\n",
    "\n",
    "# The tree structure can be traversed to compute various properties such\n",
    "# as the depth of each node and whether or not it is a leaf.\n",
    "node_depth = np.zeros(shape=n_nodes, dtype=np.int64)\n",
    "is_leaves = np.zeros(shape=n_nodes, dtype=bool)\n",
    "stack = [(0, -1)]  # seed is the root node id and its parent depth\n",
    "while len(stack) > 0:\n",
    "    node_id, parent_depth = stack.pop()\n",
    "    node_depth[node_id] = parent_depth + 1\n",
    "\n",
    "    # If we have a test node\n",
    "    if (children_left[node_id] != children_right[node_id]):\n",
    "        stack.append((children_left[node_id], parent_depth + 1))\n",
    "        stack.append((children_right[node_id], parent_depth + 1))\n",
    "    else:\n",
    "        is_leaves[node_id] = True\n",
    "\n",
    "print(\"The binary tree structure has %s nodes and has \"\n",
    "      \"the following tree structure:\"\n",
    "      % n_nodes)\n",
    "for i in range(n_nodes):\n",
    "    if is_leaves[i]:\n",
    "        print(\"%snode=%s leaf node.\" % (node_depth[i] * \"\\t\", i))\n",
    "    else:\n",
    "        print(\"%snode=%s test node: go to node %s if X[:, %s] <= %s else to \"\n",
    "              \"node %s.\"\n",
    "              % (node_depth[i] * \"\\t\",\n",
    "                 i,\n",
    "                 children_left[i],\n",
    "                 feature[i],\n",
    "                 threshold[i],\n",
    "                 children_right[i],\n",
    "                 ))\n",
    "print()\n",
    "\n",
    "# First let's retrieve the decision path of each sample. The decision_path\n",
    "# method allows to retrieve the node indicator functions. A non zero element of\n",
    "# indicator matrix at the position (i, j) indicates that the sample i goes\n",
    "# through the node j.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
